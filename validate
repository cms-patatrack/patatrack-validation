#! /bin/bash -e

# Script for testing one or more pull requests against a reference release and an intermediate development release
#
# Usage:
#   validate [PR ...]
#
# Note: this script relies on `visDQMUpload` and `visDQMUtils.py` being available in the same directory.
# If they are missing they are automatically downloaded from https://github.com/rovere/dqmgui/ .

# check if we are able to post comments on GitHub
POST_COMMENT=true
EDIT_COMMENT=true
OAUTH_TOKEN=
ISSUE_NUMBER=
COMMENT_ID=
if ! which "curl" > /dev/null; then
  echo "Command \"curl\" not found." 1>&2
  POST_COMMENT=false
fi
if ! which "jq" > /dev/null; then
  echo "Command \"jq\" not found." 1>&2
  EDIT_COMMENT=false
fi
if [ -f $HOME/.patatrack-validation.oauth ]; then
  OAUTH_TOKEN=$(< $HOME/.patatrack-validation.oauth)
else
  echo "OAUTH token not found." 1>&2
  POST_COMMENT=false
fi
[ $POST_COMMENT ] || echo "The report will not be posted to GitHub." 1>&2
[ $POST_COMMENT ] && [ $EDIT_COMMENT ] || "Only the final report will be posted to GitHub." 1>&2

# hash of this script
SCRIPTHASH=$(sha1sum ${BASH_SOURCE[0]} | cut -d' ' -f1)

BASE=$(mktemp -d -p $PWD run.XXXXXXXXXX)
[ -d $BASE ] || exit 1
REPORT=$BASE/report.md

# save the original file descriptors, then redirect all output and errors to a log file
exec 3>&1 4>&2
exec &> $BASE/log

# repository with the working branch and pull requests
REPOSITORY=cms-patatrack

# matrix workflows, default global tag and number of events
REFERENCE_WORKFLOW="10824.5"
WORKFLOWS="10824.5 10824.8 10824.7 10824.9"
PROFILING="10824.5 10824.8"
MEMCHECKS="10824.8 10824.9"
GLOBALTAG="102X_upgrade2018_realistic_v9"
NUMEVENTS=100
THREADS=1
STREAMS=1

# cuda-memcheck tools options
INITCHECK_OPTS="--track-unused-memory no"
MEMCHECK_OPTS="--leak-check full --report-api-errors all"
SYNCCHECK_OPTS=""

# datasets used as input for the matrix-like tests
SAMPLES="TTBAR ZMUMU"
TTBAR="/RelValTTbar_13/CMSSW_10_2_1-PU25ns_102X_upgrade2018_realistic_v9_gcc7-v1/GEN-SIM-DIGI-RAW"
TTBAR_NUMEVENTS=100
ZMUMU="/RelValZMM_13/CMSSW_10_2_1-102X_upgrade2018_realistic_v9_gcc7-v1/GEN-SIM-DIGI-RAW"
ZMUMU_NUMEVENTS=200

# optionally, local cache of the input files
TTBAR_CACHE_PATH="file:/data/store/relval/CMSSW_10_2_1/RelValTTbar_13/GEN-SIM-DIGI-RAW/PU25ns_102X_upgrade2018_realistic_v9_gcc7-v1/10000/"
TTBAR_CACHE_FILE="F8A80470-5C94-E811-B803-0CC47A78A3EE.root"
ZMUMU_CACHE_PATH="file:/data/store/relval/CMSSW_10_2_1/RelValZMM_13/GEN-SIM-DIGI-RAW/102X_upgrade2018_realistic_v9_gcc7-v1/10000/"
ZMUMU_CACHE_FILE="0295ACCE-5094-E811-B051-0CC47A7C3450.root"

# if not set or invalid, set VO_CMS_SW_DIR
if ! [ "$VO_CMS_SW_DIR" ] || ! [ -d "$VO_CMS_SW_DIR" ] || ! [ -f "$VO_CMS_SW_DIR/cmsset_default.sh" ]; then
  VO_CMS_SW_DIR=/data/cmssw
fi

# DQM files produced by step4
DQMFILE="DQM_V0001_R000000001__Global__CMSSW_X_Y_Z__RECO.root"

# URL and local area for uploading validation plots, profiles and logs
UPLOAD_URL="https://${USER}.web.cern.ch/${USER}/patatrack/pulls"
UPLOAD_DIR="$HOME/www/patatrack/pulls"

# URL of the DQM GUI used to upload files
DQMGUI_URL="http://dqmgui7.cern.ch:8060/dqm/dev"

# full path to visDQMUpload
DQMGUI_UPLOAD=$(dirname $(readlink -f ${BASH_SOURCE[0]}))/visDQMUpload
if ! [ -x $DQMGUI_UPLOAD ]; then
  DQMGUIREPO="https://raw.githubusercontent.com/rovere/dqmgui/index128"
  wget -q "$DQMGUIREPO/bin/visDQMUpload" "$DQMGUIREPO/src/python/DQM/visDQMUtils.py" -nd -P $(dirname $DQMGUI_UPLOAD)/
  chmod +x $DQMGUI_UPLOAD
fi

# use locally installed releases
export SCRAM_ARCH=slc7_amd64_gcc700
source $VO_CMS_SW_DIR/cmsset_default.sh


function report() {
  echo "$@" >> $REPORT
}

function can_post_comment() {
  $POST_COMMENT && [ "$ISSUE_NUMBER" ]
}

function can_update_comment() {
  can_post_comment && $EDIT_COMMENT && [ "$COMMENT_ID" ]
}

# Post a comment to a GitHub issue
#
# Usage:
#   post_comment BODY
#
# Any format accepted by curl is valid for BODY. For example,
# the format "@FILE" will use the content of FILE.
#
function post_comment() {
  can_post_comment || return
  local BODY="$1"       # body of the message, wrapped in JSON format (see https://developer.github.com/v3/issues/comments/ )
  local RESPONSE
  if ! RESPONSE=$(curl -s -S -H "Authorization: token $OAUTH_TOKEN" -X "POST" "https://api.github.com/repos/cms-patatrack/cmssw/issues/$ISSUE_NUMBER/comments" -d "$BODY"); then
    # failed; disable further posting
    POST_COMMENT=false
    EDIT_COMMENT=false
  elif $EDIT_COMMENT; then
    # success; set the comment id for further editin
    COMMENT_ID=$(echo "$RESPONSE" | jq ".id")
  fi
}

# Update a comment on a GitHub issue
#
# Usage:
#   update_comment BODY
#
# Any format accepted by curl is valid for BODY. For example,
# the format "@FILE" will use the content of FILE.
#
function update_comment() {
  can_update_comment || return
  local BODY="$1"       # body of the message, wrapped in JSON format (see https://developer.github.com/v3/issues/comments/ )
  local RESPONSE
  if ! RESPONSE=$(curl -s -S -H "Authorization: token $OAUTH_TOKEN" -X "PATCH" "https://api.github.com/repos/cms-patatrack/cmssw/issues/comments/$COMMENT_ID" -d "$BODY"); then
    # failed; disable further posting
    POST_COMMENT=false
    EDIT_COMMENT=false
  fi
}

function upload_final_report() {
  local JSON=$(mktemp -p $BASE report.XXXXXXXXXX.json)
  {
    echo -e -n '{\n    "body": "'
    cat $BASE/report.md | tr '\n' '\r' | sed -e's/\r/\\r\\n/g' -e's/"/\\"/g'
    echo -e '"\n}'
  } > $JSON
  if can_update_comment; then
    update_comment "@$JSON"
  elif can_post_comment; then
    post_comment "@$JSON"
  fi
  #rm -f $JSON
}

function upload_report() {
  local JSON=$(mktemp -p $BASE report.XXXXXXXXXX.json)
  {
    echo -e -n '{\n    "body": "'
    cat $BASE/report.md | tr '\n' '\r' | sed -e's/\r/\\r\\n/g' -e's/"/\\"/g'
    echo -e "\\\r\\\n\\\r\\\n:construction: Validation running at $(hostname):$BASE ...\"\n}"
  } > $JSON
  if can_update_comment; then
    update_comment "@$JSON"
  elif can_post_comment; then
    post_comment "@$JSON"
  fi
  #rm -f $JSON
}

function apply() {
  PATTERN="$1"; shift
    for ARG; do echo $PATTERN | sed -e"s/%/$ARG/g"; done
}

function apply_and_glob() {
  echo $(apply "$@")
}

function setup_release() {
  local DIRNAME="$1"
  local RELEASE="$2"
  # set up the reference area
  cd $BASE
  echo "# set up $DIRNAME environment for release $RELEASE"
  scram project -n $DIRNAME CMSSW $RELEASE
  cd $DIRNAME/src
  eval $(scram runtime -sh)
  git cms-init --upstream-only
  echo
  # <add here any required pull request or external update>
  #git cms-addpkg HeterogeneousCore
  #USER_CXXFLAGS="-g -rdynamic" USER_CUDA_FLAGS="-g -lineinfo" scram b -j
  git rev-parse --short=12 HEAD > ../hash
  echo
}

function setup_development_release() {
  local DIRNAME="$1"
  local RELEASE="$2"
  local BRANCH="$3"
  setup_release $DIRNAME $RELEASE
  # <add here any required pull request or external update>
  git cms-remote add $REPOSITORY
  git checkout $REPOSITORY/$BRANCH -b $BRANCH
  git cms-addpkg $(git diff $CMSSW_VERSION --name-only | cut -d/ -f-2 | sort -u) || true
  git cms-checkdeps -a
  USER_CXXFLAGS="-g -rdynamic" USER_CUDA_FLAGS="-g -lineinfo" scram b -j
  git rev-parse --short=12 HEAD > ../hash
  if [ $(git rev-parse $RELEASE) != $(git rev-parse HEAD) ]; then
    echo "# update $DIRNAME environment on branch $BRANCH with"
    git log --oneline --reverse --no-decorate ${RELEASE}..
  fi
  echo
}

function setup_patatrack_release() {
  local DIRNAME="$1"
  local RELEASE="$2"
  local BRANCH="$3"
  # set up the reference area
  cd "$BASE"
  echo "# set up $DIRNAME environment for release $RELEASE"
  scram project -n $DIRNAME CMSSW $RELEASE
  cd $DIRNAME/src
  eval $(scram runtime -sh)
  # git needs some special care
  git cms-init --upstream-only || true
  git config core.sparsecheckout true
  {
    echo "/.gitignore"
    echo "/.clang-tidy"
    echo "/.clang-format"
  } > $CMSSW_BASE/src/.git/info/sparse-checkout
  git read-tree -mu HEAD
  # <add here any required pull request or external update>
  git cms-remote add $REPOSITORY
  git checkout $REPOSITORY/$BRANCH -b $BRANCH
  git cms-addpkg $(git diff $CMSSW_VERSION --name-only | cut -d/ -f-2 | sort -u) || true
  git cms-checkdeps -a
  USER_CXXFLAGS="-g -rdynamic" USER_CUDA_FLAGS="-g -lineinfo" scram b -j
  git rev-parse --short=12 HEAD > ../hash
  if [ $(git rev-parse $RELEASE) != $(git rev-parse HEAD) ]; then
    echo "# update $DIRNAME environment on branch $BRANCH with"
    git log --oneline --reverse --no-decorate ${RELEASE}..
  fi
  echo
}

function clone_release() {
  local SOURCE="$1"
  local TARGET="$2"
  cd "$BASE"
  cp -ar "$SOURCE" "$TARGET"
  cd "$TARGET"/src
  scram b ProjectRename
  eval $(scram runtime -sh)
  echo
}

function has_profiling() {
  local WORKFLOW=$1
  echo "$PROFILING" | grep -q -w "$WORKFLOW"
}

function has_memcheck() {
  local WORKFLOW=$1
  echo "$MEMCHECKS" | grep -q -w "$WORKFLOW"
}

function build_matrix() {
  local DIRNAME="$1"
  shift
  local WORKFLOWS="$@"
  local SAMPLE DATASET WORKDIR CACHE_PATH CACHE_FILE INPUT MY_GLOBALTAG MY_NUMEVENTS WORKFLOW STEP3 STEP4
  # create the matrix-like workflows for the various samples
  cd $BASE/$DIRNAME
  eval $(scram runtime -sh)
  for SAMPLE in $SAMPLES; do
    DATASET=${!SAMPLE}
    WORKDIR=$(echo $DATASET | cut -d/ -f 2-3 --output-delimiter=-)
    CACHE_PATH=$(eval echo \$$(echo $SAMPLE)_CACHE_PATH)
    CACHE_FILE=$(eval echo \$$(echo $SAMPLE)_CACHE_FILE)
    if [ "$CACHE_PATH" ] && [ "$CACHE_FILE" ]; then
      INPUT="--dirin=$CACHE_PATH --filein $CACHE_FILE"
    else
      INPUT="--dasquery 'file dataset=$DATASET'"
    fi
    # customise the global tag and number of events by dataset, or use the default values
    MY_GLOBALTAG=$(eval echo \$$(echo $SAMPLE)_GLOBALTAG)
    MY_NUMEVENTS=$(eval echo \$$(echo $SAMPLE)_NUMEVENTS)
    echo "# prepare to run on ${MY_NUMEVENTS:=$NUMEVENTS} events on $DATASET with ${MY_GLOBALTAG:=$GLOBALTAG} conditions"
    mkdir -p $CMSSW_BASE/run/$WORKDIR
    cd $CMSSW_BASE/run/$WORKDIR
    for WORKFLOW in $WORKFLOWS; do
      # check the the workflow actually exists in the release
      runTheMatrix.py -n -e -l $WORKFLOW | grep -q ^$WORKFLOW || continue
      mkdir -p $WORKFLOW
      cd $WORKFLOW
      # extract step3 and step4 commands
      STEP3="$(runTheMatrix.py -n -e -l $WORKFLOW | grep 'cmsDriver.py step3' | cut -d: -f2- | sed -e"s/^ *//" -e"s/ \+--conditions *[^ ]\+/ --conditions $MY_GLOBALTAG/" -e"s/ \+-n *[^ ]\+/ -n $MY_NUMEVENTS/") --fileout file:step3.root"
      STEP4="$(runTheMatrix.py -n -e -l $WORKFLOW | grep 'cmsDriver.py step4' | cut -d: -f2- | sed -e"s/^ *//" -e"s/ \+--conditions *[^ ]\+/ --conditions $MY_GLOBALTAG/" -e"s/ \+-n *[^ ]\+/ -n $MY_NUMEVENTS/") --filein file:step3_inDQM.root"
      echo "# prepare workflow $WORKFLOW"
      $STEP3 $INPUT --no_exec --python_filename=step3.py
      $STEP4        --no_exec --python_filename=step4.py
      # add the NVProfilerService to step3
      sed -i step3.py -e '/\# End of customisation functions/i \
# Mark CMSSW transitions and modules in the nvprof profile\
from FWCore.ParameterSet.Utilities import moduleLabelsInSequences\
process.NVProfilerService = cms.Service("NVProfilerService",\
    highlightModules = cms.untracked.vstring( moduleLabelsInSequences(process.reconstruction_step) ),\
    showModulePrefetching = cms.untracked.bool( False )\
)\
\
# Show CUDAService messages\
process.MessageLogger.categories.append("CUDAService")\
'
      echo >> step3.py
      echo "# Configure multithreading" >> step3.py
      echo "process.options.numberOfThreads = cms.untracked.uint32( $THREADS )" >> step3.py
      echo "process.options.numberOfStreams = cms.untracked.uint32( $STREAMS )" >> step3.py

      local CUSTOMISE=RecoPixelVertexing/Configuration/customizePixelTracksForProfiling
      if has_profiling $WORKFLOW && ( [ -f $CMSSW_RELEASE_BASE/python/${CUSTOMISE}.py ] || [ -f $CMSSW_BASE/python/${CUSTOMISE}.py ] ); then
        # mark the workflow to run the profiling job
        touch .has_profiling
        # create a profiling workflow
        $STEP3 $INPUT --no_exec --customise ${CUSTOMISE}.customizePixelTracksForProfiling --python_filename=profile.py
        # add the NVProfilerService to profile
        sed -i profile.py -e '/\# End of customisation functions/i \
# Mark CMSSW transitions and modules in the nvprof profile\
from FWCore.ParameterSet.Utilities import moduleLabelsInSequences\
process.NVProfilerService = cms.Service("NVProfilerService",\
    highlightModules = cms.untracked.vstring( moduleLabelsInSequences(process.reconstruction_step) ),\
    showModulePrefetching = cms.untracked.bool( False )\
)\
\
# Show CUDAService messages\
process.MessageLogger.categories.append("CUDAService")\
'
        echo >> profile.py
        echo "# Configure multithreading" >> profile.py
        echo "process.options.numberOfThreads = cms.untracked.uint32( $THREADS )" >> profile.py
        echo "process.options.numberOfStreams = cms.untracked.uint32( $STREAMS )" >> profile.py
      fi

      if has_memcheck $WORKFLOW; then
        # mark the workflow to run the various cuda-memcheck checks
        touch .has_memcheck
      fi

      cd ..
    done
    echo
  done
}

function run_workflows() {
  local WORKDIR
  local STEP3_SUCCESS
  cd $BASE
  for WORKDIR in $(apply_and_glob "%/run/*/*/" "$@"); do
    cd $BASE/$WORKDIR
    echo "# at $WORKDIR"
    eval $(scram runtime -sh)

    # run step 3
    echo -n "# running step3... "
    if nvprof -f -o step3.nvvp -s --log-file step3.profile -- cmsRun step3.py >& step3.log; then
      STEP3_SUCCESS=true
      echo "done"
    else
      STEP3_SUCCESS=false
      echo "failed"
      tail step3.log
    fi

    # do not attempt to run step4 or profile if step3 failed
    if $STEP3_SUCCESS; then
      # run step 4
      echo -n "# running step4... "
      if cmsRun step4.py >& step4.log; then
        echo "done"
      else
        echo "failed"
        tail step4.log
      fi

      # (optional) run profile
      if [ -f .has_profiling ]; then
        echo -n "# running profile... "
        if nvprof -f -o profile.nvvp -s --log-file profile.profile -- cmsRun profile.py >& profile.log; then
          echo "done"
        else
          echo "failed"
          tail profile.log
        fi
      fi
    fi

    # (optional) run cuda-memcheck
    if [ -f .has_memcheck ]; then
      # initcheck
      echo -n "# running cuda-memcheck --tool initcheck... "
      if cuda-memcheck --tool initcheck --error-exitcode 127 $INITCHECK_OPTS --log-file cuda-initcheck.log -- cmsRun step3.py >& step3-initcheck.log; then
        echo "done"
        touch cuda-initcheck.done
      else
        echo "failed"
        tail cuda-initcheck.log
        touch cuda-initcheck.fail
      fi
      # memcheck
      echo -n "# running cuda-memcheck --tool memcheck... "
      if cuda-memcheck --tool memcheck --error-exitcode 127 $MEMCHECK_OPTS --log-file cuda-memcheck.log -- cmsRun step3.py >& step3-memcheck.log; then
        echo "done"
        touch cuda-memcheck.done
      else
        echo "failed"
        tail cuda-memcheck.log
        touch cuda-memcheck.fail
      fi
      # synccheck
      echo -n "# running cuda-memcheck --tool synccheck... "
      if cuda-memcheck --tool synccheck --error-exitcode 127 $SYNCCHECK_OPTS --log-file cuda-synccheck.log -- cmsRun step3.py >& step3-synccheck.log; then
        echo "done"
        touch cuda-synccheck.done
      else
        echo "failed"
        tail cuda-synccheck.log
        touch cuda-synccheck.fail
      fi
    fi
  done
  cd $BASE
}

# Make validation plots, based on the DQM output of step4 and makeTrackValidationPlots.py
# and upload them to the EOS www area.
#
# Usage:
#   make_validation_plots REFERENCE_RELEASE [[RELEASE ...] TARGET_RELEASE]
#
function make_validation_plots() {
  [ "$1" ] || return 1
  local DIRNAME="${!#}"
  local REFERENCE_RELEASE="$1"
  local -a RELEASES=("$@")
  shift
  local -a TESTING_RELEASES=($@)
  cd $BASE/$DIRNAME
  eval $(scram runtime -sh)

  report "### \`makeTrackValidationPlots.py\` plots"

  local SAMPLE
  for SAMPLE in $SAMPLES; do
    local DATASET=${!SAMPLE}
    report "#### $DATASET"
    local WORKDIR=$(echo $DATASET | cut -d/ -f 2-3 --output-delimiter=-)
    mkdir -p $BASE/plots/$WORKDIR
    cd $BASE/plots/$WORKDIR

    # reference release and workflow
    local FILE=$BASE/$REFERENCE_RELEASE/run/$WORKDIR/$REFERENCE_WORKFLOW/$DQMFILE
    [ -f $FILE ] && ln -s $FILE ${REFERENCE_RELEASE}-${REFERENCE_WORKFLOW}.root

    # development releases and workflows
    local RELEASE
    for RELEASE in ${TESTING_RELEASES[@]}; do
      local WORKFLOW
      for WORKFLOW in $WORKFLOWS; do
        local FILE=$BASE/$RELEASE/run/$WORKDIR/$WORKFLOW/$DQMFILE
        [ -f $FILE ] && ln -s $FILE ${RELEASE}-${WORKFLOW}.root
      done
    done

    # validation of all workflows across all releases
    local WORKFLOW
    for WORKFLOW in $WORKFLOWS; do
      mkdir -p $UPLOAD_DIR/$JOBID/$WORKDIR/$WORKFLOW
      if [ "$WORKFLOW" == "$REFERENCE_WORKFLOW" ]; then
        # reference workflow
        makeTrackValidationPlots.py \
          --extended \
          --html-sample $DATASET \
          --html-validation-name $DATASET \
          --outputDir $UPLOAD_DIR/$JOBID/$WORKDIR/$REFERENCE_WORKFLOW \
          ${RELEASES[@]/%/-${REFERENCE_WORKFLOW}.root}
        report "  - tracking validation [plots]($UPLOAD_URL/$JOBID/$WORKDIR/$WORKFLOW/index.html) and [summary]($UPLOAD_URL/$JOBID/$WORKDIR/$WORKFLOW/plots_summary.html) for workflow $WORKFLOW"
      else
        # other workflows
        local FILES=""
        local RELEASE
        for RELEASE in ${TESTING_RELEASES[@]}; do
          [ -f ${RELEASE}-${WORKFLOW}.root ] && FILES="$FILES ${RELEASE}-${WORKFLOW}.root"
        done
        if [ "$FILES" ]; then
          makeTrackValidationPlots.py \
            --html-sample $DATASET \
            --html-validation-name $DATASET \
            --outputDir $UPLOAD_DIR/$JOBID/$WORKDIR/$WORKFLOW \
            ${REFERENCE_RELEASE}-${REFERENCE_WORKFLOW}.root \
            ${TESTING_RELEASES[0]}-${REFERENCE_WORKFLOW}.root \
            $FILES
          report "  - tracking validation [plots]($UPLOAD_URL/$JOBID/$WORKDIR/$WORKFLOW/index.html) and [summary]($UPLOAD_URL/$JOBID/$WORKDIR/$WORKFLOW/plots_summary.html) for workflow $WORKFLOW"
        else
          report "  - :warning: tracking validation plots and summary for workflow $WORKFLOW are **missing**"
        fi
      fi
    done
    report
  done
}

# Build a link to the DQM GUI showing one or more test results
#
# Usage:
#   build_dqm_link REFERENCE_DATASET [DATASET ...]
#
function build_dqm_link() {
  [ "$1" ] || return
  local URL="$DQMGUI_URL/start?runnr=1;"
  URL+="dataset=$1;"
  URL+="sampletype=offline_relval;"
  URL+="filter=all;"
  URL+="referencepos=ratiooverlay;"
  URL+="referenceshow=all;"
  URL+="referencenorm=False;"
  if [ "$2" ]; then
    URL+="referenceobj1=other%3A%3A$2%3A;"
  else
    URL+="referenceobj1=none;"
  fi
  if [ "$3" ]; then
    URL+="referenceobj2=other%3A%3A$3%3A;"
  else
    URL+="referenceobj2=none;"
  fi
  if [ "$4" ]; then
    URL+="referenceobj3=other%3A%3A$4%3A;"
  else
    URL+="referenceobj3=none;"
  fi
  if [ "$5" ]; then
    URL+="referenceobj4=other%3A%3A$5%3A;"
  else
    URL+="referenceobj4=none;"
  fi
  URL+="search=;"
  URL+="striptype=object;"
  URL+="stripruns=;"
  URL+="stripaxis=run;"
  URL+="stripomit=none;"
  URL+="workspace=Everything;"
  URL+="size=L;"
  URL+="root=Tracking;"
  URL+="focus=;"
  URL+="zoom=no;"
  echo "$URL"
}

# Usage:
#   build_dqm_dataset RELEASE DATASET WORKFLOW
function build_dqm_dataset() {
  local RELEASE="$1"
  local DATASET="$2"
  local WORKFLOW="$3"
  local WORKDIR=$(echo $DATASET | cut -d/ -f 2-3 --output-delimiter=-)
  local FILE=$BASE/$RELEASE/run/$WORKDIR/$WORKFLOW/$DQMFILE
  local HASH=$(<$BASE/$RELEASE/hash)
  [ -f "$FILE" ] && echo "$(echo $DATASET | cut -d/ -f-3)-${HASH}/${WORKFLOW/./_}"
}


# Usage:
#   build_dqm_filename RELEASE DATASET WORKFLOW
function build_dqm_filename() {
  local RELEASE="$1"
  local DATASET="$2"
  local WORKFLOW="$3"
  local NAME=$(echo $DATASET | cut -d/ -f-3 | sed -e's|/|__|g')
  local HASH=$(<$BASE/$RELEASE/hash)
  echo "DQM_V0001_R000000001${NAME}-${HASH}__${WORKFLOW/./_}.root"
}


# Upload DQM plots to the GUI.
#
# Usage:
#   upload_dqm_plots REFERENCE_RELEASE [[RELEASE ...] TARGET_RELEASE]
#
function upload_dqm_plots() {
  [ "$1" ] || return 1
  local REFERENCE_RELEASE="$1"; shift
  local RELEASES=("$@")

  report "### DQM GUI plots"

  local SAMPLE
  for SAMPLE in $SAMPLES; do
    local DATASET=${!SAMPLE}
    report "#### $DATASET"
    local WORKDIR=$(echo $DATASET | cut -d/ -f 2-3 --output-delimiter=-)
    mkdir -p $BASE/dqm/$WORKDIR
    cd $BASE/dqm/$WORKDIR

    # reference release and workflow
    local DQMGUI_FILEFILE=$(build_dqm_filename $REFERENCE_RELEASE $DATASET $REFERENCE_WORKFLOW)
    local FILE=$BASE/$REFERENCE_RELEASE/run/$WORKDIR/$REFERENCE_WORKFLOW/$DQMFILE
    if [ -f $FILE ]; then
      ln -s $FILE $DQMGUI_FILEFILE
      report "  - reference [DQM plots]($(build_dqm_link $(build_dqm_dataset $REFERENCE_RELEASE $DATASET $REFERENCE_WORKFLOW))) for $REFERENCE_RELEASE release, workflow $REFERENCE_WORKFLOW"
    else
      report "  - :warning: reference DQM plots for $REFERENCE_RELEASE release, workflow $REFERENCE_WORKFLOW are **missing**"
    fi

    # development releases and workflows
    local RELEASE
    for RELEASE in ${RELEASES[@]}; do
      local WORKFLOW
      for WORKFLOW in $WORKFLOWS; do
        local DQMGUI_FILEFILE=$(build_dqm_filename $RELEASE $DATASET $WORKFLOW)
        local FILE=$BASE/$RELEASE/run/$WORKDIR/$WORKFLOW/$DQMFILE
        if [ -f $FILE ]; then
          ln -s $FILE $DQMGUI_FILEFILE
          report "  - [DQM plots]($(build_dqm_link $(build_dqm_dataset $RELEASE $DATASET $WORKFLOW))) for $RELEASE release, workflow $WORKFLOW"
        else
          report "  - :warning: DQM plots for $RELEASE release, workflow $WORKFLOW are **missing**"
        fi
      done
    done

    # upload all DQM files in the background
    $DQMGUI_UPLOAD $DQMGUI_URL DQM_*.root >& upload.log &

    # DQM-based validation of all workflows across all releases
    local WORKFLOW
    for WORKFLOW in $WORKFLOWS; do
      if [ "$WORKFLOW" == "$REFERENCE_WORKFLOW" ]; then
        # reference workflow
        local LINK=$(build_dqm_link \
          $(for RELEASE in ${REFERENCE_RELEASE} ${RELEASES[@]}; do build_dqm_dataset $RELEASE $DATASET $WORKFLOW; done) \
        )
        report "  - [DQM comparison]($LINK) for reference workflow $WORKFLOW"
      else
        # other workflows
        [ "${RELEASES[0]}" ] || continue
        local LINK=$(build_dqm_link \
          $(build_dqm_dataset ${RELEASES[0]} $DATASET $REFERENCE_WORKFLOW) \
          $(for RELEASE in ${RELEASES[@]}; do build_dqm_dataset $RELEASE $DATASET $WORKFLOW; done) \
        )
        report "  - [DQM comparison]($LINK) for workflow $WORKFLOW"
      fi
    done
    report

  done
}

# If the source file exists, dereference any symlinks amd copy it preserving mode, ownership, and timestamps
#
# Usage
#  copy_if_exists SRC DST
#
function copy_if_exists() {
  [ -f "$1" ] && cp -p -L "$1" "$2" || true
}

# Upload the output of a validation job
#
# Usage
#  upload_artefacts RELEASE WORKDIR WORKFLOW NAME
#
function upload_artefacts() {
  local RELEASE=$1
  local WORKDIR=$2
  local WORKFLOW=$3
  local CWD=$BASE/$RELEASE/run/$WORKDIR/$WORKFLOW
  report "  - $RELEASE release, workflow $WORKFLOW"

  # check the artefacts to look for and upload
  NAMES=step3
  [ -f $CWD/.has_profiling ] && NAMES="$NAMES profile"

  for NAME in $NAMES; do
    local FILE=$CWD/$NAME
    local PART=$JOBID/$WORKDIR/$RELEASE-$WORKFLOW-$NAME
    # upload the python configuration file, log, and nvprof results (if they exist)
    copy_if_exists ${FILE}.py      $UPLOAD_DIR/${PART}.py
    copy_if_exists ${FILE}.log     $UPLOAD_DIR/${PART}.log
    copy_if_exists ${FILE}.nvvp    $UPLOAD_DIR/${PART}.nvvp
    copy_if_exists ${FILE}.profile $UPLOAD_DIR/${PART}.profile
    # update the report
    if ! [ -f ${FILE}.py ]; then
      # the workflow has not been created
      report "      - ${NAME}.py: the python configuration was not created, see the full log for more information"
      continue
    fi
    if ! [ -f ${FILE}.log ]; then
      # if there is no log file, the workflow most likely did not run at all
      report "      - :warning: [${NAME}.py]($UPLOAD_URL/${PART}.py): log, profile and summary are **missing**, see the full log for more information"
      continue
    fi
    # check for both profile and summary
    if [ -f ${FILE}.nvvp ] && [ -f ${FILE}.profile ]; then
      report "      - [${NAME}.py]($UPLOAD_URL/${PART}.py): [log]($UPLOAD_URL/${PART}.log), [visual profile]($UPLOAD_URL/${PART}.nvvp) and [summary]($UPLOAD_URL/${PART}.profile)"
    elif [ -f ${FILE}.nvvp ]; then
      report "      - [${NAME}.py]($UPLOAD_URL/${PART}.py): [log]($UPLOAD_URL/${PART}.log) and [visual profile]($UPLOAD_URL/${PART}.nvvp)"
    elif [ -f ${FILE}.profile ]; then
      report "      - [${NAME}.py]($UPLOAD_URL/${PART}.py): [log]($UPLOAD_URL/${PART}.log) and [summary]($UPLOAD_URL/${PART}.profile)"
    else
      report "      - :warning: [${NAME}.py]($UPLOAD_URL/${PART}.py): [log]($UPLOAD_URL/${PART}.log); the visual profile and summary are **missing**"
    fi
  done

  # check for cuda-memcheck
  if [ -f $CWD/.has_memcheck ]; then
    local PART=$JOBID/$WORKDIR/$RELEASE-$WORKFLOW
    # cuda-memcheck --tool initcheck
    copy_if_exists $CWD/cuda-initcheck.log  $UPLOAD_DIR/$PART-cuda-initcheck.log
    copy_if_exists $CWD/step3-initcheck.log $UPLOAD_DIR/$PART-step3-initcheck.log
    local ERRORS="**some errors**"
    [ -f $CWD/cuda-initcheck.log ] && ERRORS="**$(echo $(tail -n1 $CWD/cuda-initcheck.log | cut -d: -f2 | sed -e's/========= No CUDA-MEMCHECK results found/no CUDA-MEMCHECK results/'))**"
    if [ -f $CWD/cuda-initcheck.done ]; then
      report "      - :heavy_check_mark: \`cuda-memcheck --tool initcheck $INITCHECK_OPTS\` ([report]($UPLOAD_URL/$PART-cuda-initcheck.log), [log]($UPLOAD_URL/$PART-step3-initcheck.log)) did not find any errors"
    elif [ -f $CWD/cuda-initcheck.fail ]; then
      report "      - :x: \`cuda-memcheck --tool initcheck $INITCHECK_OPTS\` ([report]($UPLOAD_URL/$PART-cuda-initcheck.log), [log]($UPLOAD_URL/$PART-step3-initcheck.log)) found $ERRORS"
    else
      report "      - :warning: \`cuda-memcheck --tool initcheck $INITCHECK_OPTS\` did not run"
    fi
    # cuda-memcheck --tool memcheck
    copy_if_exists $CWD/cuda-memcheck.log  $UPLOAD_DIR/$PART-cuda-memcheck.log
    copy_if_exists $CWD/step3-memcheck.log $UPLOAD_DIR/$PART-step3-memcheck.log
    local ERRORS="**some errors**"
    [ -f $CWD/cuda-memcheck.log ] && ERRORS="**$(echo $(tail -n1 $CWD/cuda-memcheck.log | cut -d: -f2 | sed -e's/========= No CUDA-MEMCHECK results found/no CUDA-MEMCHECK results/'))**"
    if [ -f $CWD/cuda-memcheck.done ]; then
      report "      - :heavy_check_mark: \`cuda-memcheck --tool memcheck $MEMCHECK_OPTS\` ([report]($UPLOAD_URL/$PART-cuda-memcheck.log), [log]($UPLOAD_URL/$PART-step3-memcheck.log)) did not find any errors"
    elif [ -f $CWD/cuda-memcheck.fail ]; then
      report "      - :x: \`cuda-memcheck --tool memcheck $MEMCHECK_OPTS\` ([report]($UPLOAD_URL/$PART-cuda-memcheck.log), [log]($UPLOAD_URL/$PART-step3-memcheck.log)) found $ERRORS"
    else
      report "      - :warning: \`cuda-memcheck --tool memcheck $MEMCHECK_OPTS\` did not run"
    fi
    # cuda-memcheck --tool synccheck
    copy_if_exists $CWD/cuda-synccheck.log  $UPLOAD_DIR/$PART-cuda-synccheck.log
    copy_if_exists $CWD/step3-synccheck.log $UPLOAD_DIR/$PART-step3-synccheck.log
    local ERRORS="**some errors**"
    [ -f $CWD/cuda-synccheck.log ] && ERRORS="**$(echo $(tail -n1 $CWD/cuda-synccheck.log | cut -d: -f2 | sed -e's/========= No CUDA-MEMCHECK results found/no CUDA-MEMCHECK results/'))**"
    if [ -f $CWD/cuda-synccheck.done ]; then
      report "      - :heavy_check_mark: \`cuda-memcheck --tool synccheck $SYNCCHECK_OPTS\` ([report]($UPLOAD_URL/$PART-cuda-synccheck.log), [log]($UPLOAD_URL/$PART-step3-synccheck.log)) did not find any errors"
    elif [ -f $CWD/cuda-synccheck.fail ]; then
      report "      - :x: \`cuda-memcheck --tool synccheck $SYNCCHECK_OPTS\` ([report]($UPLOAD_URL/$PART-cuda-synccheck.log), [log]($UPLOAD_URL/$PART-step3-synccheck.log)) found $ERRORS"
    else
      report "      - :warning: \`cuda-memcheck --tool synccheck $SYNCCHECK_OPTS\` did not run"
    fi
  fi
}

# Upload nvprof profiles
#
# Usage:
#   upload_profiles REFERENCE_RELEASE [[RELEASE ...] TARGET_RELEASE]
#
function upload_profiles() {
  [ "$1" ] || return 1
  local REFERENCE_RELEASE="$1"
  local -a RELEASES=("$@")
  shift
  local -a TESTING_RELEASES=($@)

  report "### logs and \`nvprof\`/\`nvvp\` profiles"

  local SAMPLE
  for SAMPLE in $SAMPLES; do
    local DATASET=${!SAMPLE}
    report "#### $DATASET"
    local WORKDIR=$(echo $DATASET | cut -d/ -f 2-3 --output-delimiter=-)

    # reference release and workflow
    upload_artefacts $REFERENCE_RELEASE $WORKDIR $REFERENCE_WORKFLOW

    # development releases and workflows
    local RELEASE
    for RELEASE in ${TESTING_RELEASES[@]}; do
      local WORKFLOW
      for WORKFLOW in $WORKFLOWS; do
        upload_artefacts $RELEASE $WORKDIR $WORKFLOW
      done
    done
    report
  done
}

# Upload the log file
#
# Usage:
#   upload_log_files REFERENCE_RELEASE [[RELEASE ...] TARGET_RELEASE]
#
# Note: the releases are used to build the hash of the upload area
#
function upload_log_files() {
  [ "$1" ] || return 1
  local -a RELEASES=("$@")

  cp $BASE/log $UPLOAD_DIR/$JOBID/log
  report "### Logs"
  report "The full log is available at $UPLOAD_URL/$JOBID/log ."
}

# main
echo > $REPORT

# upload the report to the last PR given on the command line
for ISSUE_NUMBER in "$@"; do true; done

# if we can edit the comment after posting it, create an empty comment as a starting point
can_post_comment && $EDIT_COMMENT && upload_report

# set up the reference release
report "### Validation summary"
setup_release "reference" CMSSW_10_2_2_patch1
report "Reference release [CMSSW_10_2_2_patch1](https://github.com/cms-sw/cmssw/tree/CMSSW_10_2_2_patch1) at $(< $BASE/reference/hash)"
build_matrix "reference" "$REFERENCE_WORKFLOW"
DIRECTORIES="reference"
upload_report

# set up the current development branch
setup_patatrack_release "development" CMSSW_10_2_2_Patatrack CMSSW_10_2_X_Patatrack
#setup_development_release "development" CMSSW_10_2_2_patch1 CMSSW_10_2_X_Patatrack
report "Development branch [CMSSW_10_2_X_Patatrack](https://github.com/$REPOSITORY/cmssw/tree/CMSSW_10_2_X_Patatrack) at $(< $BASE/development/hash)"
build_matrix "development" "$WORKFLOWS"
DIRECTORIES+=" development"
upload_report

# set up the development branch plus the PR(s) to be tested
if [ "$*" ]; then
  clone_release "development" "testing"
  report "Testing PRs:"
  for PR in "$@"; do
    git cms-merge-topic "$REPOSITORY:$PR"
    echo $PR >> $BASE/testing/PRs
    report "  - #$PR at $(git rev-parse --short=12 "$REPOSITORY/refs/pull/$PR/head")"
  done
  git rev-parse --short=12 HEAD > ../hash
  USER_CXXFLAGS="-g -rdynamic" USER_CUDA_FLAGS="-g -lineinfo" scram b -j
  build_matrix "testing" "$WORKFLOWS"
  DIRECTORIES+=" testing"
fi
report
upload_report

# compute a unique hash for this validation run
JOBID=$({ echo $SCRIPTHASH; cat $(apply_and_glob "$BASE/%/hash" $DIRECTORIES); } | sha1sum | cut -d' ' -f1)
echo $JOBID > $BASE/jobid

# run the workflows
run_workflows $DIRECTORIES
upload_report
# make validation plots
make_validation_plots $DIRECTORIES
upload_report
# upload DQM plots to the GUI
upload_dqm_plots $DIRECTORIES
upload_report
# upload nvprof profiles
upload_profiles $DIRECTORIES
upload_report

# restore the original descriptors, close and upload the log files
exec 1>&3- 2>&4-
upload_log_files $DIRECTORIES

echo "This report can be found at $REPORT:"
echo
cat $REPORT

# post the report to GitHub
upload_final_report
